{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERN2_Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In the name of Allah"
      ],
      "metadata": {
        "id": "IOHD9B7eEnzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addr to uptodate paragraphs data"
      ],
      "metadata": {
        "id": "Tq6LgEpHHaWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utd_parag_addr = '/content/drive/MyDrive/UNIVERSITY/Term 3/NLP/NLP_Project/NLP_Final_Project/data/UptoDate_parags.zip'"
      ],
      "metadata": {
        "id": "7Xbftby4HZz2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '$utd_parag_addr'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwgYJynmFcWA",
        "outputId": "d70c316d-6e66-4e4c-9c33-2b562be2a83a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/UNIVERSITY/Term 3/NLP/NLP_Project/NLP_Final_Project/data/UptoDate_parags.zip\n",
            "  inflating: parag.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package imports"
      ],
      "metadata": {
        "id": "RGgSJR5NEsei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "rPuIRoa2ErnS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addr to pickle file where annotations are going to be saved (or saved before to load and continue annotation)"
      ],
      "metadata": {
        "id": "_E3e6a_1Ez7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annot_pickle_addr = './annots_list.pickle'"
      ],
      "metadata": {
        "id": "s6kjiy5VEyxC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addr to pickle file which has a list of papers id to annotate (None to annotate all the papers)"
      ],
      "metadata": {
        "id": "lfjeyofDFVek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "portion_id_addr = None"
      ],
      "metadata": {
        "id": "PwB-H0J3FdXL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to query BERN2 server with exception handling."
      ],
      "metadata": {
        "id": "iWR7VBSPFB5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_plain(text, url=\"http://bern2.korea.ac.kr/plain\"):\n",
        "    trys = 0\n",
        "    while(True):\n",
        "        try:\n",
        "            return requests.post(url, json={'text': text}).json()\n",
        "        except Exception as e:\n",
        "            print(F\"Error in request: {str(e)}\")\n",
        "            print('Sleep for 1 seconds')\n",
        "            time.sleep(1)\n",
        "            if 'Expecting value' in str(e):\n",
        "                trys += 1\n",
        "                text = re.sub(r'www\\.\\w+\\.\\w+', '', text)\n",
        "            if trys > 10:\n",
        "                return {'annotations':[]}"
      ],
      "metadata": {
        "id": "Z2NkAn61FBoF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main procedure to annotate paragraphs"
      ],
      "metadata": {
        "id": "JzUzyIPYFjIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load portion_id file and get papers id which you want to get annotate.\n",
        "# If there is no portion_id file, this means you want to annotate all of the papers\n",
        "if portion_id_addr != None:\n",
        "  if os.path.exists(portion_id_addr):\n",
        "    with open(portion_id_addr, 'rb') as f:\n",
        "        portion_id = pickle.load(f)\n",
        "  else:\n",
        "    portion_id = None\n",
        "else:\n",
        "  portion_id = None\n",
        "\n",
        "# Load previously annotated data\n",
        "if os.path.exists(annot_pickle_addr):\n",
        "    with open(annot_pickle_addr, 'rb') as f:\n",
        "        l = pickle.load(f)\n",
        "else:\n",
        "    l=[]\n",
        "\n",
        "# Get paper ids which processed before\n",
        "proccesed_papers_id=[]\n",
        "for row in l:\n",
        "    if row[0] not in proccesed_papers_id:\n",
        "        proccesed_papers_id.append(row[0])\n",
        "\n",
        "# Load uptodate paragraphs\n",
        "df_iter = pd.read_csv('/content/parag.csv').iterrows()\n",
        "i=0\n",
        "for ind, row in tqdm(df_iter):\n",
        "    paper_id = row['paper_id']\n",
        "    if paper_id in proccesed_papers_id or (portion_id != None and paper_id not in portion_id):\n",
        "        continue\n",
        "    paragraph_id = row['parag_id']\n",
        "\n",
        "    if i>180:\n",
        "        i=0\n",
        "        with open(annot_pickle_addr, 'wb') as f:\n",
        "            pickle.dump(l, f)\n",
        "        print('Sleep for 5 seconds')\n",
        "        time.sleep(5)\n",
        "\n",
        "    # If paragpraph length is less than 5000, query whole paragpraph.\n",
        "    # Else break paragraph into sentences and query text with less than 5000 characters\n",
        "    if len(row['text']) < 5000:\n",
        "        annotations = query_plain(row['text'])['annotations']\n",
        "        i+=1\n",
        "        for annot in annotations:\n",
        "            l.append([paper_id, paragraph_id, annot['mention'],\n",
        "                      annot['id'], annot['obj'], annot['prob'],\n",
        "                      annot['span']['begin'], annot['span']['end']])\n",
        "    else:\n",
        "        sentcs = row['text'].split('.')\n",
        "        query = \"\"\n",
        "        qslen = 0\n",
        "        for sentc in sentcs:\n",
        "            if len(query +'. '+ sentc) < 5000:\n",
        "                query = query + sentc + '. '\n",
        "            else:\n",
        "                annotations = query_plain(query)['annotations']\n",
        "                i += 1\n",
        "                for annot in annotations:\n",
        "                    l.append([paper_id, paragraph_id, annot['mention'],\n",
        "                              annot['id'], annot['obj'], annot['prob'],\n",
        "                              qslen+annot['span']['begin'], qslen+annot['span']['end']])\n",
        "                qslen += len(query)\n",
        "                query = \"\"\n",
        "        else:\n",
        "            if query != \"\":\n",
        "                annotations = query_plain(query)['annotations']\n",
        "                i += 1\n",
        "                query = \"\"\n",
        "                for annot in annotations:\n",
        "                    l.append([paper_id, paragraph_id, annot['mention'],\n",
        "                              annot['id'], annot['obj'], annot['prob'],\n",
        "                              qslen+annot['span']['begin'], qslen+annot['span']['end']])\n",
        "\n",
        "# Last pickle dump\n",
        "with open(annot_pickle_addr, 'wb') as f:\n",
        "    pickle.dump(l, f)"
      ],
      "metadata": {
        "id": "ETedGGm5OY69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dataframe from annotations"
      ],
      "metadata": {
        "id": "inSi_KIViDKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bern_df = pd.DataFrame(l, columns=['paper_id', 'parag_id', 'mention', 'object_id', 'object_type', 'prob', 'begin_ind', 'end_ind'])"
      ],
      "metadata": {
        "id": "43XWvjr9h1Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get unique annotations and save it as pickle"
      ],
      "metadata": {
        "id": "pVZJJ2bdiIBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_mentions = list(pd.unique(bern_df['mention']))\n",
        "\n",
        "with open('all_unique_mentions.pickle', 'wb') as f:\n",
        "    pickle.dump(unique_mentions,f)"
      ],
      "metadata": {
        "id": "AXCLgpS_iOBZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}