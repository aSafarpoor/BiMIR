{"cells":[{"cell_type":"markdown","metadata":{"id":"QD6kvumyNyFS"},"source":["# In the name of Allah"]},{"cell_type":"markdown","source":["### Please input data addrs:"],"metadata":{"id":"PvnhYCcDBADd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVyFlRxGOIoq"},"outputs":[],"source":["# list of data which should be normalized:\n","bijankhan_zip_addr = \"'Bijankhan.rar'\"\n","\n","persianHealth_addr = \"'persian_health.zip'\"\n","\n","utdParags_addr = \"UptoDate_parags.zip\"\n","\n","wikiEn_addr = \"en_papers.json\"\n","wikiFa_addr = \"fa_papers.json\""]},{"cell_type":"markdown","source":["### Unzip data"],"metadata":{"id":"TCcwZA1lLIhR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2D1f-3RMD4I"},"outputs":[],"source":["# first level data is zipped or rarred so we ask to unzip\n","!unrar x '$bijankhan_zip_addr'\n","!unzip -q $persianHealth_addr\n","!unzip -q $bijankhan_data_addr\n","!unzip -q $utdParags_addr\n","!rm /content/health/README.md\n","!rm -r /content/health/.ipynb_checkpoints"]},{"cell_type":"markdown","source":["### Install required package"],"metadata":{"id":"iSo2g-7GoBC8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10820,"status":"ok","timestamp":1645093611908,"user":{"displayName":"Alireza Sahebi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicspjLu5qIdriM63UAuvCNy-3gjMj8YsBBnjJ1MQ=s64","userId":"16001976437513665461"},"user_tz":-210},"id":"2sX2hDjoRfR5","outputId":"0e03b270-3fb5-4d16-d157-335f7b7ee823"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 316 kB 13.9 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 57.8 MB/s \n","\u001b[K     |████████████████████████████████| 233 kB 64.9 MB/s \n","\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip -q install hazm"]},{"cell_type":"markdown","source":["### Import required packages"],"metadata":{"id":"xvKTIUODKJh7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":986,"status":"ok","timestamp":1645099144513,"user":{"displayName":"Alireza Sahebi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicspjLu5qIdriM63UAuvCNy-3gjMj8YsBBnjJ1MQ=s64","userId":"16001976437513665461"},"user_tz":-210},"id":"o4zoNMbpi96I","outputId":"38ede79c-2eec-4e42-b047-ebe322efab50"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":105}],"source":["import re\n","import pandas as pd\n","import nltk\n","import pickle\n","import tqdm\n","import json \n","import pandas as pd\n","import numpy as np\n","import os\n","from hazm import Normalizer, SentenceTokenizer\n","from nltk.tokenize import sent_tokenize\n","from itertools import chain\n","#it can be use if we want punctuations but not all the time, in this code we used ready one\n","nltk.download('punkt')"]},{"cell_type":"markdown","source":["### Define normalization functions:"],"metadata":{"id":"s5XKvow4w_zO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eT3ZJL_xYt9T"},"outputs":[],"source":["#partially based on https://stackoverflow.com/questions/41174566/how-to-normalize-persian-texts-with-hazm\n","\n","def en_remove_punc(s):\n","    #list of punctuations, more complete than ready lists\n","    punc = '\"#\\'*+,-/:;<=>@[\\]^_`{|}~\\'●,•()»«–‑-،؛−٫—' # \"●,•\" not happen in common texts but in our data we saw them sometimes\n","    table = str.maketrans(dict.fromkeys(punc, ' ')) \n","    new_s = s.translate(table) \n","    new_s = ' '.join(new_s.split())\n","    return new_s\n","\n","\n","def fa_remove_punc(s):\n","    punc = '\"#\\'*+,-:;<=>@[\\]^_`{|}~\\'●,•()»«–‑-،؛−—'\n","    table = str.maketrans(dict.fromkeys(punc, ' ')) \n","    new_s = s.translate(table) \n","    new_s = ' '.join(new_s.split())\n","    return new_s\n","\n","def en_normalizer(text):\n","    # using lower characters is common in different english tasks\n","    text = text.lower()\n","    # remove \\xa0 which is useless in our project\n","    text = text.replace('\\xa0','')\n","    #text = text.replace('-',' ')\n","    # removeing citations from text which is not usable here\n","    text = re.sub(r\"\\[[\\d| ]+\\]\", \" \", text)\n","    text = en_remove_punc(text)\n","    #text = re.sub(r\"(.)\\.([^0-9]|\\n|$)\", r\"\\1 . \\2\", text)\n","    text = re.sub(r\"(\\w{2,}| )\\.([^0-9]|\\n|$)\", r\"\\1 . \\2\", text)\n","    # here convert commom puctuations into based format, as example latin to english, or in persian from english to persian\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\?\", \" ? \", text)\n","    text = re.sub(r\"؟\", \" ؟ \", text)\n","    text = re.sub(r\" +\", \" \", text)\n","    return text\n","\n","\n","def fa_normalizer(text):\n","    text = arToPersianChar(text)\n","    text = arToPersianNumb(text)\n","    text = text.replace('\\xa0','')\n","    #text = text.replace('-',' ')\n","    text = text.replace('ٔ', '')\n","    text = fa_remove_punc(text)\n","    # more_normalization_function()\n","    normalizer = Normalizer(persian_style = False, punctuation_spacing = False, affix_spacing = False)\n","    text = normalizer.normalize(text)\n","    # this two next line are usable in persian texts, but it is common to see different behavioural from writers, sometimes using them and sometimes no.\n","    text = text.replace('\\u200c',' ')\n","    text = text.replace('\\u200b',' ')\n","    #here we manage '.'s in end of sentences, but it can understand to avoid float numbers dots. \n","    text = re.sub(r\"(\\w{2,}| )\\.([^0-9]|\\n|$)\", r\"\\1 . \\2\", text)\n","    # still same as above comments, convert signs to thier normal forms\n","    text = re.sub(r'([\\d+])\\.([\\d+])', r'\\1٫\\2', text)\n","    text = re.sub(r'([\\d+])/([\\d+])', r'\\1٫\\2', text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\?\", \" ? \", text)\n","    text = re.sub(r\" +\", \" \", text)\n","    text = re.sub(r\" +\", \" \", text)\n","    return text\n","\n","\n","def arToPersianNumb(number):\n","    dic = {\n","        '١': '۱',\n","        '٢': '۲',\n","        '٣': '۳',\n","        '٤': '۴',\n","        '٥': '۵',\n","        '٦': '۶',\n","        '٧': '۷',\n","        '٨': '۸',\n","        '٩': '۹',\n","        '٠': '۰',\n","    }\n","    return multiple_replace(dic, number)\n","\n","\n","def arToPersianChar(userInput):\n","    dic = {\n","        'ك': 'ک',\n","        'دِ': 'د',\n","        'بِ': 'ب',\n","        'زِ': 'ز',\n","        'ذِ': 'ذ',\n","        'شِ': 'ش',\n","        'سِ': 'س',\n","        'ى': 'ی',\n","        'ي': 'ی'\n","    }\n","    return multiple_replace(dic, userInput)\n","\n","def multiple_replace(dic, text):\n","    # it help us to have less line of codes because it do different replaces in one line\n","    pattern = \"|\".join(map(re.escape, dic.keys()))\n","    return re.sub(pattern, lambda m: dic[m.group()], str(text))"]},{"cell_type":"markdown","source":["### Normalize English and Farsi wikipedia data"],"metadata":{"id":"Fy9BumCuPHjP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOBsx_a4Wgja"},"outputs":[],"source":["with open(wikiFa_addr) as f:\n","  wikiFa_data = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAik8svlS4wO"},"outputs":[],"source":["with open(wikiEn_addr) as f:\n","  wikiEn_data = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVrCWjjWRUTX"},"outputs":[],"source":["wiki_en_df = pd.DataFrame(wikiEn_data.items(), columns=['Key', 'Value'])"]},{"cell_type":"code","source":["wiki_en_parags_normd = []\n","deleted = []\n","for _, doc in tqdm.tqdm(wiki_en_df.iterrows()):\n","  # each '\\n' means new paragraph, so it is a good splitting criteria!\n","  for parag in doc['Value'].split('\\n'):\n","    parag_normd = en_normalizer(parag)\n","    if parag_normd == '':\n","      deleted.append(parag)\n","      continue\n","    wiki_en_parags_normd.append(parag_normd)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLvKDwIPsO7h","executionInfo":{"status":"ok","timestamp":1645129079130,"user_tz":-210,"elapsed":105927,"user":{"displayName":"Alireza Sahebi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicspjLu5qIdriM63UAuvCNy-3gjMj8YsBBnjJ1MQ=s64","userId":"16001976437513665461"}},"outputId":"e3bb894a-2b3f-4dfb-9842-43715c57c505"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["27019it [01:46, 253.58it/s]\n"]}]},{"cell_type":"code","source":["wiki_fa_df = pd.DataFrame(wikiFa_data.items(), columns=['Key', 'Value'])"],"metadata":{"id":"N9jLh1nT5WyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wiki_fa_parags_normd = []\n","deleted_fa = []\n","for _, doc in tqdm.tqdm(wiki_fa_df.iterrows()):\n","  for parag in doc['Value'].split('\\n'):\n","    parag_normd = fa_normalizer(parag)\n","    if parag_normd == '':\n","      deleted_fa.append(parag)\n","      continue\n","    wiki_fa_parags_normd.append(parag_normd)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D7Cplf1w49lM","executionInfo":{"status":"ok","timestamp":1645128973220,"user_tz":-210,"elapsed":60024,"user":{"displayName":"Alireza Sahebi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicspjLu5qIdriM63UAuvCNy-3gjMj8YsBBnjJ1MQ=s64","userId":"16001976437513665461"}},"outputId":"f8812d53-f30b-46c9-8c97-0e6625f5e46b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["16506it [00:59, 278.50it/s]\n"]}]},{"cell_type":"markdown","source":["#### Save wikipedia normalized data"],"metadata":{"id":"MYBXjbzgE6yt"}},{"cell_type":"code","source":["with open('./wiki-en_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for text in wiki_en_parags_normd[:-1]:\n","    f.write(text+'\\n')\n","  else:\n","    f.write(wiki_en_parags_normd[-1])"],"metadata":{"id":"H1DG86UPNPVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./wiki-fa_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for text in wiki_fa_parags_normd[:-1]:\n","    f.write(text+'\\n')\n","  else:\n","    f.write(wiki_fa_parags_normd[-1])"],"metadata":{"id":"7hPLhQwgNPwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Normalize English and Farsi wikipedia titles"],"metadata":{"id":"joiG0VdhE_sl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pefS5yLT0co"},"outputs":[],"source":["with open(\"./orphans_merged.json\") as f:\n","  orphans_merged = json.load(f)\n","om = []\n","for i in orphans_merged:\n","    om.append(en_normalizer(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4klGtZk5UHvY"},"outputs":[],"source":["with open('./orphans_merged_normd.txt', 'w') as f:\n","    for item in om:\n","        f.write(\"%s\\n\" % item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNm-UOSkUbY1"},"outputs":[],"source":["with open(\"./pairs_merged.json\") as f:\n","  pairs_merged = json.load(f)\n","pm = []\n","for i in pairs_merged:\n","    pm.append([en_normalizer(i[0]),fa_normalizer(i[1])])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAe8UE8tX0F7"},"outputs":[],"source":["with open('./pairs_merged_normd.txt', 'w') as f:\n","    for item in pm:\n","        f.write(\"%s\\n\" % item)"]},{"cell_type":"markdown","source":["### Normalize Farsi Bijankhan data"],"metadata":{"id":"ZAlKyjWjPLy4"}},{"cell_type":"code","source":["with open(bijankhan_processed_addr, 'r') as f:\n","  bjk = f.read()"],"metadata":{"id":"8cF1Qy9_POsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Get text from POS tagged bijankhan data. Then get paragraphs of text which are seperated by \\# sign "],"metadata":{"id":"tCLc25UBFRtd"}},{"cell_type":"code","source":["bjk_splt = bjk.split('\\n')\n","bjk_splt_splt = [row.split('  ')[0] for row in bjk_splt]\n","bjk_text = ' '.join(bjk_splt_splt)\n","bjk_text_splt = bjk_text.split(\"#\")"],"metadata":{"id":"ZJDXW0jdTump"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bjk_text_splt_normd = []\n","for doc in bjk_text_splt:\n","  bjk_text_splt_normd.append(fa_normalizer(doc))"],"metadata":{"id":"YHId-175V7gp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Save normalized data"],"metadata":{"id":"qt1r3m9sGzE4"}},{"cell_type":"code","source":["with open('./bijankhan_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for text in bjk_text_splt_normd[:-1]:\n","    f.write(text+'\\n')\n","  else:\n","    f.write(bjk_text_splt_normd[-1])"],"metadata":{"id":"KMM0FisgM2HH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalize Farsi health data from Namnak and HiDoctor websites (from course github exploring datasets)"],"metadata":{"id":"ah6DUqShi7jb"}},{"cell_type":"code","source":["fa_health_jsons = [os.path.join('health',x) for x in os.listdir('health')]"],"metadata":{"id":"36LkrH99jH7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(fa_health_jsons[0], 'r') as f:\n","  text = json.load(f)"],"metadata":{"id":"XON94i-1Yh38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fa_health_parags_normd = []\n","for js in fa_health_jsons:\n","  with open(js, 'r') as f:\n","    text = json.load(f)\n","    for doc in text:\n","      for parag in doc['paragraphs']:\n","        fa_health_parags_normd.append(fa_normalizer(parag))"],"metadata":{"id":"RkdQAAanjtIs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Save normalized data"],"metadata":{"id":"PG379dcFJH8X"}},{"cell_type":"code","source":["with open('./persian-health_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for text in fa_health_parags_normd[:-1]:\n","    f.write(text+'\\n')\n","  else:\n","    f.write(fa_health_parags_normd[-1])"],"metadata":{"id":"ZQ0G5OSWWz66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalize UptoDate English papers"],"metadata":{"id":"eyI9D_V43jBo"}},{"cell_type":"code","source":["utd_parags_df = pd.read_csv(utdParags_addr)"],"metadata":{"id":"aZ-1qMg53ijY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["utd_parags_normd = list(utd_parags_df['text'].apply(en_normalizer))"],"metadata":{"id":"f512fOta4bu-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Save normalized data"],"metadata":{"id":"dPLnuBghJa85"}},{"cell_type":"code","source":["with open('./uptodate_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for text in utd_parags_normd[:-1]:\n","    f.write(text+'\\n')\n","  else:\n","    f.write(utd_parags_normd[-1])"],"metadata":{"id":"Swhop9zVBWip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Write all English and Farsi data in seperate files"],"metadata":{"id":"oB8lAb-hMIW4"}},{"cell_type":"code","source":["all_en_parags = [wiki_en_parags_normd, utd_parags_normd]\n","all_fa_parags = [bjk_text_splt_normd, fa_health_parags_normd, wiki_fa_parags_normd]"],"metadata":{"id":"zO0130JQjUKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./all-en_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for dateset in all_en_parags:\n","    for text in dateset:\n","      f.write(text+'\\n')"],"metadata":{"id":"2pWtJcLjMGmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./all-fa_parags_normd.txt', 'w', encoding = 'UTF-8') as f:\n","  for dateset in all_fa_parags:\n","    for text in dateset:\n","      f.write(text+'\\n')"],"metadata":{"id":"CiupIPEajW2O"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Editted_Normalization.ipynb","provenance":[{"file_id":"1cM2X9wBAtChq7MLPpBFVbsSEGB81rEJb","timestamp":1645203516654}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}